{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T16:41:15.844547Z",
     "start_time": "2025-04-02T16:41:15.839478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "from numpy.f2py.auxfuncs import throw_error"
   ],
   "id": "c5256ab460800cf9",
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-05T13:10:02.765062Z",
     "start_time": "2025-04-05T13:10:02.761983Z"
    }
   },
   "source": [
    "class ActivationFunction(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x):\n",
    "        raise Exception(\"Not implemented call method in activation function.\")\n",
    "\n",
    "\n",
    "    def ddx(self, x):\n",
    "        raise Exception(\"Not implemented d/dx method in activation function.\")\n",
    "\n",
    "class ReLu(ActivationFunction):\n",
    "    def __call__(self, x : float):\n",
    "        return max(0 ,x)\n",
    "    def ddx(self, x):\n",
    "        return 1 if x>=0 else 0\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 449
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:10:04.200234Z",
     "start_time": "2025-04-05T13:10:04.196440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ErrorFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def ddx(self):\n",
    "        pass\n",
    "\n",
    "class MSE(ErrorFunction):\n",
    "    def __call__(self, y_pred: list, y_true : list):\n",
    "        pass"
   ],
   "id": "dd0c0a9e404cd698",
   "outputs": [],
   "execution_count": 450
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:44:14.002031Z",
     "start_time": "2025-04-05T13:44:13.988813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, x):\n",
    "        pass\n",
    "\n",
    "    # def deriff(self, x):\n",
    "    #     pass\n",
    "    #\n",
    "    #\n",
    "    # def ddw_i(self):\n",
    "    #     raise Exception(\"Nie zaimplementowano, d/dw_i\")\n",
    "    #\n",
    "    #\n",
    "    # def ddb(self):\n",
    "    #     raise Exception(\"Nie zaimplementowano d/db\")\n",
    "\n",
    "\n",
    "class Perceptron(Layer):\n",
    "\n",
    "    def __init__(self, input_size, initialization_type : str = \"he\"):\n",
    "        self.weight = [random.gauss(mu = 0, sigma=math.sqrt(2/input_size)) for i in range(input_size)]\n",
    "        self.bias = random.gauss(mu = 0, sigma=0.1)\n",
    "\n",
    "        self.grad_weight = [0 for i in range(len(self.weight)) ]\n",
    "        self.grad_bias = 0\n",
    "        self.backward_grad = [0 for i in range(len(self.weight)) ] # grad z poprzedniego\n",
    "\n",
    "        self.batch_grad_weight = [0 for i in range(len(self.weight)) ]\n",
    "        self.batch_grad_bias = 0\n",
    "\n",
    "        self.forward_value = None\n",
    "        self.forward_x = None\n",
    "\n",
    "\n",
    "        self.weight = [2 for i in range(input_size)]\n",
    "        self.bias = 2\n",
    "        # self.activate_function = ReLu()\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        if len(x) != len(self.weight):\n",
    "            raise Exception(\"Size od weights not match input size.\")\n",
    "        output = [ self.weight[i] * x[i] for i in range(len(x)) ]\n",
    "        output.append(self.bias)\n",
    "        return sum(output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.forward_value = self(x)\n",
    "        self.forward_x = x\n",
    "        return self.forward_value\n",
    "\n",
    "\n",
    "\n",
    "    def zero_batch_grad(self):\n",
    "        self.batch_grad_weight = [0 for i in range(len(self.weight)) ]\n",
    "        self.batch_grad_bias = 0\n",
    "    def zero_grad(self):\n",
    "        self.grad_weight = [0 for i in range(len(self.weight)) ]\n",
    "        self.grad_bias = 0\n",
    "        self.backward_grad = [0 for i in range(len(self.weight)) ] # grad z poprzedniego\n",
    "\n",
    "\n",
    "\n",
    "    def ddw_i(self, i):\n",
    "        return self.forward_x[i]\n",
    "    def ddx_i(self,i):\n",
    "        return self.weight[i]\n",
    "\n",
    "    def ddw(self, previous_grad): # x w tym przypadku jest previous_grad\n",
    "        for i in range (len(self.grad_weight)):\n",
    "            self.grad_weight[i] += self.ddw_i(i)*previous_grad\n",
    "    def ddb(self, previous_grad):\n",
    "        self.grad_bias += previous_grad\n",
    "    def calculate_backward_grad(self, previous_grad):\n",
    "        self.backward_grad = ([self.ddx_i(i)*previous_grad  for i in range(len(self.grad_weight))])\n",
    "\n",
    "\n",
    "\n",
    "    def add_grad_to_batch_grad(self):\n",
    "        self.batch_grad_bias += self.grad_bias\n",
    "        for i in range(len(self.grad_weight)):\n",
    "            self.batch_grad_weight[i] += self.grad_weight[i]\n",
    "    def normalize_batch_grad(self, batch_size : int):\n",
    "        self.batch_grad_bias /= batch_size\n",
    "        for i in range(len(self.grad_weight)):\n",
    "            self.batch_grad_weight[i] /= batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self,input_size : int, output_size : int, activation_function : str):\n",
    "\n",
    "        self.activation_function = None\n",
    "        self.layer = []\n",
    "\n",
    "\n",
    "\n",
    "        match activation_function:\n",
    "            case \"ReLu\":\n",
    "                self.activation_function = ReLu()\n",
    "                self.layer = [Perceptron(input_size = input_size) for i in range(output_size)]\n",
    "            # case \"Sigmoid\":\n",
    "            #     self.activation_function = ReLu()\n",
    "            #     self.layers = [Perceptron() for i in range(output_size)]\n",
    "            # case \"Tanh\":\n",
    "            #     self.activation_function = ReLu()\n",
    "            #     self.layers = [Perceptron() for i in range(output_size)]\n",
    "            # case \"LeakyReLu\":\n",
    "            #     self.activation_function = ReLu()\n",
    "            #     self.layers = [Perceptron() for i in range(output_size)]\n",
    "            case _ :\n",
    "                warnings.warn(f\"{activation_function} is not a activation function.\")\n",
    "                self.activation_function = lambda x: x\n",
    "                self.layer = [Perceptron(input_size=input_size) for i in range(output_size)]\n",
    "\n",
    "    def __call__(self, inputs : list):\n",
    "        outputs = [perceptron(inputs) for perceptron in self.layer]\n",
    "        return [self.activation_function(output) for output in outputs]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = [perceptron.forward(inputs) for perceptron in self.layer]\n",
    "        return [self.activation_function(output) for output in outputs]\n",
    "\n",
    "\n",
    "    def zero_batch_grad(self):\n",
    "        for perceptron in self.layer:\n",
    "            perceptron.zero_batch_grad()\n",
    "    def zero_grad(self):\n",
    "        for perceptron in self.layer:\n",
    "            perceptron.zero_grad()\n",
    "    def add_grad_to_batch_grad(self):\n",
    "        for perceptron in self.layer:\n",
    "            perceptron.add_grad_to_batch_grad()\n",
    "    def normalize_batch_grad(self, batch_size : int):\n",
    "        for perceptron in self.layer:\n",
    "            perceptron.normalize_batch_grad(batch_size)\n",
    "\n",
    "\n",
    "    # def ddx_i(self, i:int):\n",
    "    #     return [ perceptron.ddx_i(i) for perceptron in self.layer]\n",
    "    # def d_activation_dx_i(self, i):\n",
    "    #     return self.activation_function.ddx(self.layer[i].value)\n",
    "\n",
    "\n",
    "\n",
    "    def print_forward(self):\n",
    "        print([perceptron.forward_value for perceptron in self.layer])\n",
    "    def print_grad(self):\n",
    "        print([[perceptron.grad_weight, perceptron.grad_bias ]for perceptron in self.layer])\n",
    "    def print_backward_grad(self):\n",
    "        print([perceptron.backward_grad for perceptron in self.layer])\n",
    "\n",
    "    def print_batch_grad(self):\n",
    "        print([[perceptron.batch_grad_weight, perceptron.batch_grad_bias ]for perceptron in self.layer])\n",
    "\n"
   ],
   "id": "d63d8016ba57ea6f",
   "outputs": [],
   "execution_count": 520
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:44:36.689497Z",
     "start_time": "2025-04-05T13:44:36.677983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = [Linear(2,2,\"ReLu\"),\n",
    "                      Linear(2,2, \"ReLu\"),\n",
    "                      Linear(2,1, \"ReLu\")]\n",
    "    def __call__(self, x_original : list):\n",
    "        x = deepcopy(x_original)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_original):\n",
    "        x = deepcopy(x_original)\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def zero_batch_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_batch_grad()\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "    def add_grad_to_batch_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.add_grad_to_batch_grad()\n",
    "    def normalize_batch_grad(self, batch_size : int):\n",
    "        for layer in self.layers:\n",
    "            layer.normalize_batch_grad(batch_size)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.zero_grad()\n",
    "\n",
    "        for perceptron in self.layers[len(self.layers)-1].layer:\n",
    "            x = perceptron.forward_value\n",
    "            previous_grad = self.layers[len(self.layers)-1].activation_function.ddx(x)\n",
    "            perceptron.ddb(previous_grad)\n",
    "            perceptron.ddw(previous_grad )\n",
    "            perceptron.calculate_backward_grad( previous_grad)\n",
    "\n",
    "        self.layers.reverse()\n",
    "        try:\n",
    "            for i in range(1, len(self.layers)):\n",
    "                linear = self.layers[i]\n",
    "                previous_linear = self.layers[i-1]\n",
    "                for j in range(len(linear.layer)):\n",
    "                    perceptron = linear.layer[j]\n",
    "                    for k in range(len(previous_linear.layer)):\n",
    "                        previous_perceptron = previous_linear.layer[k]\n",
    "                        previous_grad = previous_perceptron.backward_grad[j]\n",
    "                        # previous_grad *= d_a\n",
    "                        # print(previous_grad)\n",
    "                        # 1/0\n",
    "\n",
    "                        perceptron.ddb(previous_grad )\n",
    "                        perceptron.ddw(previous_grad ) # TODO: domnóż to przez pochodną f aktywacji\n",
    "                        perceptron.calculate_backward_grad( previous_grad )\n",
    "\n",
    "        except Exception as e:\n",
    "            self.layers.reverse()\n",
    "            raise e\n",
    "\n",
    "        self.layers.reverse()\n",
    "\n",
    "\n",
    "\n",
    "    def print_values(self, type:str):\n",
    "        self.layers.reverse()\n",
    "        try:\n",
    "            match type:\n",
    "                case \"backward_grad\":\n",
    "                    for layer in self.layers:\n",
    "                        print(\"Backward_grad\")\n",
    "                        layer.print_backward_grad()\n",
    "\n",
    "                case \"grad\":\n",
    "                    for layer in self.layers:\n",
    "                        print(\"Grads:\")\n",
    "                        layer.print_grad()\n",
    "\n",
    "                case \"forward\":\n",
    "                    for layer in self.layers:\n",
    "                        print(\"Forward\")\n",
    "                        layer.print_forward()\n",
    "\n",
    "                case \"batch\":\n",
    "                    for layer in self.layers:\n",
    "                        print(\"Batch_rads:\")\n",
    "                        layer.print_batch_grad()\n",
    "                case \"all\":\n",
    "\n",
    "                    print(\"Backward_grad\")\n",
    "                    for layer in self.layers:\n",
    "                        layer.print_backward_grad()\n",
    "                    print(\"Grads:\")\n",
    "                    for layer in self.layers:\n",
    "                        layer.print_grad()\n",
    "\n",
    "                    print(\"Batch_grad:\")\n",
    "                    for layer in self.layers:\n",
    "                        layer.print_batch_grad()\n",
    "\n",
    "                    print(\"Forward\")\n",
    "                    for layer in self.layers:\n",
    "                        layer.print_forward()\n",
    "                case _:\n",
    "                    print(f\"{type} is not correct type of print.\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            self.layers.reverse()\n",
    "            raise e\n",
    "\n",
    "        self.layers.reverse()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model()"
   ],
   "id": "604b16a8813489b0",
   "outputs": [],
   "execution_count": 523
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:44:37.010434Z",
     "start_time": "2025-04-05T13:44:37.008859Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c82ab5cc4b33dcef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:44:37.449918Z",
     "start_time": "2025-04-05T13:44:37.447628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model([1,2]))\n",
    "print(model.forward([1,2]))\n",
    "model.backward()"
   ],
   "id": "e32467107cdc83f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138]\n",
      "[138]\n"
     ]
    }
   ],
   "execution_count": 524
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:44:38.103568Z",
     "start_time": "2025-04-05T13:44:38.100565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.add_grad_to_batch_grad()\n",
    "model.add_grad_to_batch_grad()\n",
    "model.add_grad_to_batch_grad()\n",
    "model.normalize_batch_grad(3)"
   ],
   "id": "f1630608776c1ec2",
   "outputs": [],
   "execution_count": 525
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:44:38.665259Z",
     "start_time": "2025-04-05T13:44:38.663067Z"
    }
   },
   "cell_type": "code",
   "source": "model.print_values(\"all\")",
   "id": "97495d9d9e4ef811",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward_grad\n",
      "[[2, 2]]\n",
      "[[4, 4], [4, 4]]\n",
      "[[8, 8], [8, 8]]\n",
      "Grads:\n",
      "[[[34, 34], 1]]\n",
      "[[[16, 16], 2], [[16, 16], 2]]\n",
      "[[[8, 16], 8], [[8, 16], 8]]\n",
      "Batch_grad:\n",
      "[[[34.0, 34.0], 1.0]]\n",
      "[[[16.0, 16.0], 2.0], [[16.0, 16.0], 2.0]]\n",
      "[[[8.0, 16.0], 8.0], [[8.0, 16.0], 8.0]]\n",
      "Forward\n",
      "[138]\n",
      "[34, 34]\n",
      "[8, 8]\n"
     ]
    }
   ],
   "execution_count": 526
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:23:30.877235Z",
     "start_time": "2025-04-05T13:23:30.874306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(model.layers)):\n",
    "    number_layer = i\n",
    "    print(i,model.layers[number_layer].layer[0].backward_grad, model.layers[number_layer].layer[0].grad_weight, model.layers[number_layer].layer[0].grad_bias)"
   ],
   "id": "4116e01b1f75c825",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [8, 8] [8, 16] 8\n",
      "1 [4, 4] [16, 16] 2\n",
      "2 [2, 2] [34, 34] 1\n"
     ]
    }
   ],
   "execution_count": 476
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T17:12:47.304801Z",
     "start_time": "2025-04-02T17:12:47.301826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = Perceptron(2)\n",
    "a([1,2])"
   ],
   "id": "e7b908f3a8bc3e6f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.188139308930177"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T17:13:02.356072Z",
     "start_time": "2025-04-02T17:13:02.353485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a.forward([1,2])\n",
    "# print(a.value)"
   ],
   "id": "a06177ca50b7831e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.188139308930177\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T23:14:06.794500Z",
     "start_time": "2025-03-29T23:14:06.791657Z"
    }
   },
   "cell_type": "code",
   "source": "a.bias",
   "id": "a2c0347f8d36ac7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08249214176724436"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T11:37:03.231793Z",
     "start_time": "2025-04-02T11:37:03.229942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = None\n",
    "warnings.warn(f\"its {a}\")"
   ],
   "id": "890df32265413530",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/w406vwt16zl0wwv25rzht4fc0000gn/T/ipykernel_68565/3845773530.py:2: UserWarning: its None\n",
      "  warnings.warn(f\"its {a}\")\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T21:28:11.589204Z",
     "start_time": "2025-04-02T21:28:11.586903Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5c203e1f6dc8544f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T21:28:12.088760Z",
     "start_time": "2025-04-02T21:28:12.087361Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7680f55dc54d18dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T22:03:38.401889Z",
     "start_time": "2025-04-02T22:03:38.399719Z"
    }
   },
   "cell_type": "code",
   "source": "print([1,2].reverse())",
   "id": "8ac91be27a2bb7dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef55ca51461db939"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
