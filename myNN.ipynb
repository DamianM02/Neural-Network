{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:38:12.787912Z",
     "start_time": "2025-04-05T19:38:12.785108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from numpy.f2py.auxfuncs import throw_error"
   ],
   "id": "c5256ab460800cf9",
   "outputs": [],
   "execution_count": 438
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-05T20:06:18.404509Z",
     "start_time": "2025-04-05T20:06:18.399168Z"
    }
   },
   "source": [
    "class ActivationFunction(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: list):\n",
    "        raise Exception(\"Not implemented call method in activation function.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def ddx(self, x:list):\n",
    "        raise Exception(\"Not implemented d/dx method in activation function.\")\n",
    "\n",
    "\n",
    "class ReLu(ActivationFunction):\n",
    "    def __call__(self, x: list):\n",
    "        return [max(0, x_i) for x_i in x]\n",
    "\n",
    "    def ddx(self, x : list):\n",
    "\n",
    "        return [1 if x_i >= 0 else 0 for x_i in x]\n",
    "\n",
    "\n",
    "class SoftMax(ActivationFunction):\n",
    "\n",
    "    # def __init__(self, output_size: int):\n",
    "\n",
    "    def __call__(self, x:list):\n",
    "        # return x\n",
    "        ex = [math.exp(x_i) for x_i in x]\n",
    "        return [ex_i / sum(ex) for ex_i in ex]\n",
    "\n",
    "    def ddx(self, x:list):\n",
    "        # return [[1,0],[0,1]]\n",
    "        ex = [math.exp(x_i) for x_i in x]\n",
    "        ddx=[]\n",
    "        for i in range(len(x)):\n",
    "            ddx.append([ ex[i]*(1-ex[i])/sum(ex)**2 if i == j else ex[i]*ex[j]/sum(ex)**2 for j in range(len(x)) ])\n",
    "        print(ddx)\n",
    "        return ddx\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 542
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:06:18.832418Z",
     "start_time": "2025-04-05T20:06:18.829431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer(ABC):\n",
    "    pass\n",
    "\n",
    "class SGD(Optimizer):\n"
   ],
   "id": "25d3834c1a0b3adc",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1547674395.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[543], line 5\u001B[0;36m\u001B[0m\n\u001B[0;31m    \u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m incomplete input\n"
     ]
    }
   ],
   "execution_count": 543
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:06:19.292037Z",
     "start_time": "2025-04-05T20:06:19.286475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LossFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def ddx(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MSE(LossFunction):\n",
    "\n",
    "    def __init__(self, sample_reduntion : Literal[\"mean\", \"sum\"] = \"mean\", batch_reduction : Literal[\"mean\", \"sum\"] = \"mean\"):\n",
    "        self.sample_reduction = sample_reduntion\n",
    "        self.batch_reduction = batch_reduction\n",
    "\n",
    "    def __call__(self, y_pred: list[list], y_true: list[list]):\n",
    "        if len(y_pred)!=len(y_true):\n",
    "            raise Exception(f\"y_pred len is {len(y_pred)} not y_true len {len(y_true)}.\")\n",
    "\n",
    "\n",
    "        batch_loss = 0\n",
    "        for y_pred_sample, y_true_sample in zip(y_pred, y_true):\n",
    "\n",
    "            if len(y_pred_sample)!=len(y_true_sample):\n",
    "                raise Exception(f\"In some sample y_pred_sample len is {len(y_pred_sample)} not y_true_sample len {len(y_true_sample)}.\")\n",
    "\n",
    "            sample_loss = 0\n",
    "            for y_pred_sample_value, y_true_sample_value in zip(y_pred_sample, y_true_sample):\n",
    "                sample_loss += (y_pred_sample_value - y_true_sample_value)**2\n",
    "            if self.sample_reduction == \"mean\":\n",
    "                sample_loss/=len(y_true_sample_value)\n",
    "            batch_loss += sample_loss\n",
    "        if self.batch_reduction == \"mean\":\n",
    "            batch_loss /=len(y_pred)\n",
    "        return batch_loss\n",
    "\n",
    "    def batch_forward_backward( self, model, y_true : list[list], X: list[list]):\n",
    "\n",
    "        if len(y_true)!=len(X):\n",
    "            raise Exception(f\"y_pred len is {len(y_true)} not y_true len {len(X)}.\")\n",
    "\n",
    "        model.zero_batch_grad()\n",
    "\n",
    "        for y_true_sample, X_sample in zip(y_true, X):\n",
    "            self.sample_forward_backward(model, y_true_sample, X_sample)\n",
    "            model.add_grad_to_batch_grad()\n",
    "\n",
    "        if self.batch_reduction == \"mean\":\n",
    "            model.normalize_batch_grad(len(y_true))\n",
    "\n",
    "\n",
    "    def sample_forward_backward(self, model, y_true_sample, X_sample):\n",
    "        y_pred_sample = model(X_sample)\n",
    "\n",
    "\n",
    "        if len(y_pred_sample) != len(y_true_sample):\n",
    "            raise Exception(f\"y_pred len is {len(y_pred_sample)}, not y_true len {len(y_true_sample)}\")\n",
    "\n",
    "        model.forward(X_sample)\n",
    "        loss_grad = [2*(y_pred_sample[i] - y_true_sample[i]) for i in range(len(y_pred_sample))]\n",
    "\n",
    "\n",
    "        if self.sample_reduction == \"mean\":\n",
    "            loss_grad = [loss_grad[i] / len(y_pred_sample) for i in range (len(loss_grad))]\n",
    "\n",
    "        # print(loss_grad)\n",
    "        model.backward(loss_grad)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "dd0c0a9e404cd698",
   "outputs": [],
   "execution_count": 544
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:08:19.131272Z",
     "start_time": "2025-04-05T20:08:19.114537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation_function : ActivationFunction =None\n",
    "        self.layer :list[Perceptron] = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = [perceptron.forward(inputs) for perceptron in self.layer]\n",
    "        return self.activation_function(outputs)\n",
    "\n",
    "    def get_forward(self):\n",
    "        return [perceptron.forward_value for perceptron in self.layer]\n",
    "    def print_forward(self):\n",
    "        print(self.get_forward())\n",
    "\n",
    "    def print_grad(self):\n",
    "        print([[perceptron.grad_weight, perceptron.grad_bias] for perceptron in self.layer])\n",
    "\n",
    "    def print_backward_grad(self):\n",
    "        print([perceptron.backward_grad for perceptron in self.layer])\n",
    "\n",
    "    def print_batch_grad(self):\n",
    "        print([[perceptron.batch_grad_weight, perceptron.batch_grad_bias] for perceptron in self.layer])\n",
    "\n",
    "\n",
    "    def zero_batch_grad(self):\n",
    "        for perceptron in self.layer:\n",
    "            perceptron.zero_batch_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for perceptron in self.layer:\n",
    "            perceptron.zero_grad()\n",
    "\n",
    "    def add_grad_to_batch_grad(self):\n",
    "        for perceptron in self.layer:\n",
    "            perceptron.add_grad_to_batch_grad()\n",
    "\n",
    "    def normalize_batch_grad(self, batch_size: int):\n",
    "        for perceptron in self.layer:\n",
    "            perceptron.normalize_batch_grad(batch_size)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Perceptron():\n",
    "\n",
    "    def __init__(self, input_size, initialization_type: str = \"he\"):\n",
    "\n",
    "        self.bias = None\n",
    "        self.weight = None\n",
    "\n",
    "        match initialization_type:\n",
    "            case \"he\":\n",
    "                self.weight: list[float] = [random.gauss(mu=0, sigma=math.sqrt(2 / input_size)) for i in range(input_size)]\n",
    "                self.bias: float = random.gauss(mu=0, sigma=math.sqrt(2 / input_size))\n",
    "            case \"glorot\" | \"xavier\":\n",
    "                self.weight: list[float] = [random.gauss(mu=0, sigma=math.sqrt(1 / input_size)) for i in range(input_size)]\n",
    "                self.bias: float = random.gauss(mu=0, sigma=math.sqrt(1 / input_size))\n",
    "            case _:\n",
    "                print(f\"{initialization_type} is not right initialization_type\")\n",
    "\n",
    "\n",
    "        self.grad_weight: list[float] = [0 for i in range(len(self.weight))]\n",
    "        self.grad_bias: float = 0\n",
    "        self.backward_grad: list[float] = [0 for i in range(len(self.weight))]  # gradient for following perceptron\n",
    "\n",
    "        self.batch_grad_weight: list[float] = [0 for i in range(len(self.weight))]\n",
    "        self.batch_grad_bias: float = 0\n",
    "\n",
    "        self.forward_value: float = None\n",
    "        self.forward_x: list[float] = None\n",
    "\n",
    "        # self.weight = [2 for i in range(input_size)]\n",
    "        # self.bias = 2\n",
    "        # self.activate_function = ReLu()\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        if len(x) != len(self.weight):\n",
    "            raise Exception(\"Size od weights not match input size.\")\n",
    "        output = [self.weight[i] * x[i] for i in range(len(x))]\n",
    "        output.append(self.bias)\n",
    "        return sum(output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.forward_value = self(x)\n",
    "        self.forward_x = x\n",
    "        return self.forward_value\n",
    "\n",
    "    def zero_batch_grad(self):\n",
    "        self.batch_grad_weight = [0 for i in range(len(self.weight))]\n",
    "        self.batch_grad_bias = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad_weight = [0 for i in range(len(self.weight))]\n",
    "        self.grad_bias = 0\n",
    "        self.backward_grad = [0 for i in range(len(self.weight))]  # grad z poprzedniego\n",
    "\n",
    "    def ddw_i(self, i):\n",
    "        return self.forward_x[i]\n",
    "\n",
    "    def ddx_i(self, i):\n",
    "        return self.weight[i]\n",
    "\n",
    "    def ddw(self, previous_grad):\n",
    "        for i in range(len(self.grad_weight)):\n",
    "            self.grad_weight[i] += self.ddw_i(i) * previous_grad\n",
    "\n",
    "    def ddb(self, previous_grad):\n",
    "        self.grad_bias += previous_grad\n",
    "\n",
    "    def calculate_backward_grad(self, previous_grad):\n",
    "\n",
    "        # self.backward_grad = ([self.backward_grad[i] + self.ddx_i(i) * previous_grad for i in range(len(self.grad_weight))])\n",
    "\n",
    "        for i in range(len(self.backward_grad)):\n",
    "            self.backward_grad[i] += self.ddx_i(i) * previous_grad\n",
    "\n",
    "    def add_grad_to_batch_grad(self):\n",
    "        self.batch_grad_bias += self.grad_bias\n",
    "        for i in range(len(self.grad_weight)):\n",
    "            self.batch_grad_weight[i] += self.grad_weight[i]\n",
    "\n",
    "    def normalize_batch_grad(self, batch_size: int):\n",
    "        self.batch_grad_bias /= batch_size\n",
    "        for i in range(len(self.grad_weight)):\n",
    "            self.batch_grad_weight[i] /= batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "class Softmax_layer(Layer):\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        self.activation_function = SoftMax()\n",
    "        self.layer = [Perceptron(input_size= input_size) for i in range(output_size)]\n",
    "\n",
    "    def __call__(self, inputs:list ):\n",
    "        outputs = [perceptron(inputs) for perceptron in self.layer]\n",
    "        return self.activation_function(outputs)\n",
    "\n",
    "    def backward_from_loss_function(self, losses_grads : list[float]):\n",
    "        # print(self.get_forward())\n",
    "        activation_function_grad = self.activation_function.ddx(self.get_forward())\n",
    "        # print(\"xd\",activation_function_grad)\n",
    "        for i in range(len(losses_grads)):\n",
    "            losses_grad = losses_grads[i]\n",
    "            # print(losses_grad)\n",
    "            for j in range(len(self.layer)):\n",
    "                perceptron = self.layer[j]\n",
    "                perceptron.ddb(losses_grad * activation_function_grad[i][j])\n",
    "                perceptron.ddw(losses_grad * activation_function_grad[i][j])\n",
    "                perceptron.calculate_backward_grad(losses_grad * activation_function_grad[i][j])\n",
    "                print(perceptron.backward_grad, perceptron.grad_weight, perceptron.grad_bias)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int, activation_function: str):\n",
    "\n",
    "        self.activation_function: ActivationFunction = None\n",
    "        self.layer: list[Perceptron] = []\n",
    "\n",
    "        match activation_function:\n",
    "            case \"ReLu\":\n",
    "                self.activation_function = ReLu()\n",
    "                self.layer = [Perceptron(input_size=input_size, initialization_type=\"he\") for i in range(output_size)]\n",
    "            # case \"Sigmoid\":\n",
    "            #     self.activation_function = ReLu()\n",
    "            #     self.layers = [Perceptron() for i in range(output_size)]\n",
    "            # case \"Tanh\":\n",
    "            #     self.activation_function = ReLu()\n",
    "            #     self.layers = [Perceptron() for i in range(output_size)]\n",
    "            # case \"LeakyReLu\":\n",
    "            #     self.activation_function = ReLu()\n",
    "            #     self.layers = [Perceptron() for i in range(output_size)]\n",
    "\n",
    "            # case \"SoftMax\":\n",
    "            #     self.activation_function = SoftMax()\n",
    "            #     self.layer = [Perceptron(input_size=input_size) for i in range(output_size)]\n",
    "\n",
    "            case _:\n",
    "                warnings.warn(f\"{activation_function} is not a activation function.\")\n",
    "                self.activation_function = lambda x: x\n",
    "                self.layer = [Perceptron(input_size=input_size) for i in range(output_size)]\n",
    "\n",
    "    def __call__(self, inputs: list):\n",
    "        outputs = [perceptron(inputs) for perceptron in self.layer]\n",
    "        return self.activation_function(outputs)\n",
    "        # return [self.activation_function(output) for output in outputs]\n",
    "\n",
    "\n",
    "    # def forward(self, inputs):\n",
    "    #     outputs = [perceptron.forward(inputs) for perceptron in self.layer]\n",
    "    #     return self.activation_function(outputs)\n",
    "\n",
    "    def backward_from_loss_function(self, losses_grads : list[float]):\n",
    "        for perceptron, previous_grad, d_activation_dx in zip(self.layer,losses_grads, self.activation_function.ddx(self.get_forward())):\n",
    "            perceptron.ddb(previous_grad)\n",
    "            perceptron.ddw(previous_grad)\n",
    "            perceptron.calculate_backward_grad(previous_grad)\n",
    "\n",
    "\n",
    "    def backward_from_previous_layer(self, previous_layer: \"Linear\"):\n",
    "        activation_function_grad = self.activation_function.ddx(self.get_forward())\n",
    "\n",
    "        for previous_perceptron in previous_layer.layer:\n",
    "            self.backward_from_previous_perceptron(previous_perceptron, activation_function_grad)\n",
    "\n",
    "    def backward_from_previous_perceptron(self, previous_perceptron: Perceptron, activation_function_grad : list[float]):\n",
    "        for perceptron, previous_grad, d_activation_dx in zip( self.layer, previous_perceptron.backward_grad, activation_function_grad):\n",
    "            previous_grad = previous_grad * d_activation_dx\n",
    "            perceptron.ddb(previous_grad )\n",
    "            perceptron.ddw(previous_grad )\n",
    "            perceptron.calculate_backward_grad(previous_grad)\n",
    "\n",
    "    # def zero_batch_grad(self):\n",
    "    #     for perceptron in self.layer:\n",
    "    #         perceptron.zero_batch_grad()\n",
    "    #\n",
    "    # def zero_grad(self):\n",
    "    #     for perceptron in self.layer:\n",
    "    #         perceptron.zero_grad()\n",
    "    #\n",
    "    # def add_grad_to_batch_grad(self):\n",
    "    #     for perceptron in self.layer:\n",
    "    #         perceptron.add_grad_to_batch_grad()\n",
    "    #\n",
    "    # def normalize_batch_grad(self, batch_size: int):\n",
    "    #     for perceptron in self.layer:\n",
    "    #         perceptron.normalize_batch_grad(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    # def get_forward(self):\n",
    "    #     return [perceptron.forward_value for perceptron in self.layer]\n",
    "    # def print_forward(self):\n",
    "    #     print(self.get_forward())\n",
    "    #\n",
    "    # def print_grad(self):\n",
    "    #     print([[perceptron.grad_weight, perceptron.grad_bias] for perceptron in self.layer])\n",
    "    #\n",
    "    # def print_backward_grad(self):\n",
    "    #     print([perceptron.backward_grad for perceptron in self.layer])\n",
    "    #\n",
    "    # def print_batch_grad(self):\n",
    "    #     print([[perceptron.batch_grad_weight, perceptron.batch_grad_bias] for perceptron in self.layer])\n",
    "\n"
   ],
   "id": "d63d8016ba57ea6f",
   "outputs": [],
   "execution_count": 549
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:08:19.965878Z",
     "start_time": "2025-04-05T20:08:19.954824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers: list[Layer] = [Linear(2, 2, \"ReLu\"),\n",
    "                                    Linear(2, 2, \"ReLu\"),\n",
    "                                    Softmax_layer(2,2)]\n",
    "\n",
    "    def __call__(self, x_original: list):\n",
    "        x = deepcopy(x_original)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_original):\n",
    "        x = deepcopy(x_original)\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def zero_batch_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_batch_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "\n",
    "    def add_grad_to_batch_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.add_grad_to_batch_grad()\n",
    "\n",
    "    def normalize_batch_grad(self, batch_size: int):\n",
    "        for layer in self.layers:\n",
    "            layer.normalize_batch_grad(batch_size)\n",
    "\n",
    "    # def backward(self):\n",
    "    #     self.zero_grad()\n",
    "    #\n",
    "    #     for perceptron in self.layers[len(self.layers) - 1].layer:\n",
    "    #         x = perceptron.forward_value\n",
    "    #         previous_grad = self.layers[len(self.layers) - 1].activation_function.ddx(x)\n",
    "    #         perceptron.ddb(previous_grad)\n",
    "    #         perceptron.ddw(previous_grad)\n",
    "    #         perceptron.calculate_backward_grad(previous_grad)\n",
    "    #\n",
    "    #     self.layers.reverse()\n",
    "    #     try:\n",
    "    #         for i in range(1, len(self.layers)):\n",
    "    #             linear = self.layers[i]\n",
    "    #             previous_linear = self.layers[i - 1]\n",
    "    #             for j in range(len(linear.layer)):\n",
    "    #                 perceptron = linear.layer[j]\n",
    "    #                 for k in range(len(previous_linear.layer)):\n",
    "    #                     previous_perceptron = previous_linear.layer[k]\n",
    "    #                     previous_grad = previous_perceptron.backward_grad[j]* linear.activation_function.ddx(perceptron.forward_value)\n",
    "    #                     # previous_grad *= d_a\n",
    "    #                     # print(previous_grad)\n",
    "    #                     # 1/0\n",
    "    #\n",
    "    #                     perceptron.ddb(previous_grad)\n",
    "    #                     perceptron.ddw(previous_grad)  # TODO: domnóż to przez pochodną f aktywacji\n",
    "    #                     perceptron.calculate_backward_grad(previous_grad)\n",
    "    #\n",
    "    #     except Exception as e:\n",
    "    #         self.layers.reverse()\n",
    "    #         raise e\n",
    "    #\n",
    "    #     self.layers.reverse()\n",
    "\n",
    "    def backward(self, loss_grad:list[float] = None):\n",
    "        self.zero_grad()\n",
    "\n",
    "        linear = self.layers[-1]\n",
    "\n",
    "        if loss_grad is None:\n",
    "            loss_grad = [1 for i in range(len(linear.layer))]\n",
    "\n",
    "\n",
    "        # activation_function_grad = linear.activation_function.ddx(linear.get_forward())\n",
    "        # for perceptron, loss_grad, d_activation_dx  in zip( linear.layer, loss_grad, activation_function_grad):\n",
    "        #\n",
    "        #     previous_grad = d_activation_dx *loss_grad\n",
    "        #     perceptron.ddb(previous_grad)\n",
    "        #     perceptron.ddw(previous_grad)\n",
    "        #     perceptron.calculate_backward_grad(previous_grad)\n",
    "\n",
    "        self.layers.reverse()\n",
    "        try:\n",
    "            linear = self.layers[0]\n",
    "\n",
    "            linear.backward_from_loss_function(loss_grad)\n",
    "\n",
    "            for i in range(1, len(self.layers)):\n",
    "                linear = self.layers[i]\n",
    "                previous_linear = self.layers[i - 1]\n",
    "\n",
    "                linear.backward_from_previous_layer(previous_linear)\n",
    "\n",
    "                # for j in range(len(linear.layer)):\n",
    "                #     perceptron = linear.layer[j]\n",
    "                #     for k in range(len(previous_linear.layer)):\n",
    "                #         previous_perceptron = previous_linear.layer[k]\n",
    "                #         previous_grad = previous_perceptron.backward_grad[j]* linear.activation_function.ddx(perceptron.forward_value)\n",
    "                #         # previous_grad *= d_a\n",
    "                #         # print(previous_grad)\n",
    "                #         # 1/0\n",
    "                #\n",
    "                #         perceptron.ddb(previous_grad)\n",
    "                #         perceptron.ddw(previous_grad)  # TODO: domnóż to przez pochodną f aktywacji\n",
    "                #         perceptron.calculate_backward_grad(previous_grad)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.layers.reverse()\n",
    "            raise e\n",
    "\n",
    "        self.layers.reverse()\n",
    "\n",
    "\n",
    "    # def backward_from_loss(self, loss_grad:list[float] = None):\n",
    "    #     self.zero_grad()\n",
    "    #\n",
    "    #     if loss_grad is None:\n",
    "    #         loss_grad = [1 for i in range(len(self.layers[-1].layer))]\n",
    "    #\n",
    "    #\n",
    "    #     for perceptron, loss_grad in zip( self.layers[len(self.layers) - 1].layer, loss_grad):\n",
    "    #         x = perceptron.forward_value\n",
    "    #         previous_grad = self.layers[len(self.layers) - 1].activation_function.ddx(x)*loss_grad\n",
    "    #         perceptron.ddb(previous_grad)\n",
    "    #         perceptron.ddw(previous_grad)\n",
    "    #         perceptron.calculate_backward_grad(previous_grad)\n",
    "    #\n",
    "    #     self.layers.reverse()\n",
    "    #     try:\n",
    "    #         for i in range(1, len(self.layers)):\n",
    "    #             linear = self.layers[i]\n",
    "    #             previous_linear = self.layers[i - 1]\n",
    "    #             for j in range(len(linear.layer)):\n",
    "    #                 perceptron = linear.layer[j]\n",
    "    #                 for k in range(len(previous_linear.layer)):\n",
    "    #                     previous_perceptron = previous_linear.layer[k]\n",
    "    #                     previous_grad = previous_perceptron.backward_grad[j]* linear.activation_function.ddx(perceptron.forward_value)\n",
    "    #                     # previous_grad *= d_a\n",
    "    #                     # print(previous_grad)\n",
    "    #                     # 1/0\n",
    "    #\n",
    "    #                     perceptron.ddb(previous_grad)\n",
    "    #                     perceptron.ddw(previous_grad)  # TODO: domnóż to przez pochodną f aktywacji\n",
    "    #                     perceptron.calculate_backward_grad(previous_grad)\n",
    "    #\n",
    "    #     except Exception as e:\n",
    "    #         self.layers.reverse()\n",
    "    #         raise e\n",
    "    #\n",
    "    #     self.layers.reverse()\n",
    "\n",
    "    def print_values(self, type: str):\n",
    "        self.layers.reverse()\n",
    "        try:\n",
    "            match type:\n",
    "                case \"backward_grad\":\n",
    "                    for layer in self.layers:\n",
    "                        print(\"Backward_grad\")\n",
    "                        layer.print_backward_grad()\n",
    "\n",
    "                case \"grad\":\n",
    "                    for layer in self.layers:\n",
    "                        print(\"Grads:\")\n",
    "                        layer.print_grad()\n",
    "\n",
    "                case \"forward\":\n",
    "                    for layer in self.layers:\n",
    "                        print(\"Forward\")\n",
    "                        layer.print_forward()\n",
    "\n",
    "                case \"batch\":\n",
    "                    for layer in self.layers:\n",
    "                        print(\"Batch_rads:\")\n",
    "                        layer.print_batch_grad()\n",
    "                case \"all\":\n",
    "\n",
    "                    print(\"Backward_grad\")\n",
    "                    for layer in self.layers:\n",
    "                        layer.print_backward_grad()\n",
    "                    print(\"Grads:\")\n",
    "                    for layer in self.layers:\n",
    "                        layer.print_grad()\n",
    "\n",
    "                    print(\"Batch_grad:\")\n",
    "                    for layer in self.layers:\n",
    "                        layer.print_batch_grad()\n",
    "\n",
    "                    print(\"Forward\")\n",
    "                    for layer in self.layers:\n",
    "                        layer.print_forward()\n",
    "                case _:\n",
    "                    print(f\"{type} is not correct type of print.\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            self.layers.reverse()\n",
    "            raise e\n",
    "\n",
    "        self.layers.reverse()\n",
    "\n",
    "\n",
    "model = Model()\n",
    "criterion = MSE()"
   ],
   "id": "604b16a8813489b0",
   "outputs": [],
   "execution_count": 550
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:08:20.481788Z",
     "start_time": "2025-04-05T20:08:20.478846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_sample = [1,2]\n",
    "y_true_sample = [137, 137]\n",
    "y_pred_sample = model(X_sample)\n",
    "y_pred_sample"
   ],
   "id": "6da8ce23d0aa2337",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.004314354318419335, 0.9956856456815807]"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 551
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:08:20.979539Z",
     "start_time": "2025-04-05T20:08:20.977390Z"
    }
   },
   "cell_type": "code",
   "source": "criterion.batch_forward_backward(model, [y_true_sample], [X_sample])",
   "id": "dc4076cb977fe128",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.002302932891856047, 0.004295740665234472], [0.004295740665234472, -0.45561319311990994]]\n",
      "[-0.04184717820465668, 0.5407900663257422] [-0.15388457102309394, -0.9248821283873824] -0.31549187051581146\n",
      "[-0.5550566197662258, -0.24298478818241737] [-0.28704623214760533, -1.7252147396532291] -0.5884979377898328\n",
      "[-0.11934129729947271, 1.5422446829354013] [-0.4388535888956373, -2.6376123262372126] -0.8997311343349894\n",
      "[57.88912988095613, 25.34187947909323] [29.937228064463692, 179.92971632045146] 61.37686200326518\n"
     ]
    }
   ],
   "execution_count": 552
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:08:22.291207Z",
     "start_time": "2025-04-05T20:08:22.288344Z"
    }
   },
   "cell_type": "code",
   "source": "model.print_values(\"all\")",
   "id": "11dedac339a23359",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward_grad\n",
      "[[-0.11934129729947271, 1.5422446829354013], [57.88912988095613, 25.34187947909323]]\n",
      "[[19.496456824878184, -37.831724934770726], [20.551617122377387, -9.86862872174609]]\n",
      "[[31.911667724248193, 112.52818793596916], [-78.51578431975193, 13.42318528315888]]\n",
      "Grads:\n",
      "[[[-0.4388535888956373, -2.6376123262372126], -0.8997311343349894], [[29.937228064463692, 179.92971632045146], 61.37686200326518]]\n",
      "[[[246.64669918050464, 50.084538010163094], 57.76978858365666], [[114.78111046435643, 23.307666021890064], 26.88412416202863]]\n",
      "[[[40.048073947255574, 80.09614789451115], 40.048073947255574], [[-47.70035365651682, -95.40070731303364], -47.70035365651682]]\n",
      "Batch_grad:\n",
      "[[[-0.4388535888956373, -2.6376123262372126], -0.8997311343349894], [[29.937228064463692, 179.92971632045146], 61.37686200326518]]\n",
      "[[[246.64669918050464, 50.084538010163094], 57.76978858365666], [[114.78111046435643, 23.307666021890064], 26.88412416202863]]\n",
      "[[[40.048073947255574, 80.09614789451115], 40.048073947255574], [[-47.70035365651682, -95.40070731303364], -47.70035365651682]]\n",
      "Forward\n",
      "[-4.826093503174633, 0.6153904109874868]\n",
      "[0.48776081225643414, 2.9315561344742487]\n",
      "[4.269475537777581, 0.8669676527833484]\n"
     ]
    }
   ],
   "execution_count": 553
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:51:09.702408Z",
     "start_time": "2025-04-05T19:51:09.700095Z"
    }
   },
   "cell_type": "code",
   "source": "model.print_values(\"all\")",
   "id": "c82ab5cc4b33dcef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward_grad\n",
      "[[0.0, 0.0], [0.0, 0.0]]\n",
      "[[0.0, 0.0], [0.0, 0.0]]\n",
      "[[0.0, 0.0], [0.0, 0.0]]\n",
      "Grads:\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "Batch_grad:\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "Forward\n",
      "[138, 138]\n",
      "[34, 34]\n",
      "[8, 8]\n"
     ]
    }
   ],
   "execution_count": 502
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:51:10.573679Z",
     "start_time": "2025-04-05T19:51:10.570064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_sample_new = [1,1]\n",
    "y_pred_sample_new = model(X_sample)\n",
    "y_true_sample_new = [105,106]\n",
    "\n",
    "criterion.batch_forward_backward(model, [y_true_sample, y_true_sample_new], [X_sample, X_sample_new])\n",
    "model.print_values(\"all\")"
   ],
   "id": "2adffc6e80d2d9bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-136.5\n",
      "-136.5\n",
      "-104.5\n",
      "-105.5\n",
      "Backward_grad\n",
      "[[-2.3521237068610334e+92, -2.3521237068610334e+92], [2.3521237068610334e+92, 2.3521237068610334e+92]]\n",
      "[[0.0, 0.0], [0.0, 0.0]]\n",
      "[[0.0, 0.0], [0.0, 0.0]]\n",
      "Grads:\n",
      "[[[-3.0577608189193546e+93, -3.0577608189193546e+93], -1.1760618534305167e+92], [[3.0577608189193546e+93, 3.0577608189193546e+93], 1.1760618534305167e+92]]\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "Batch_grad:\n",
      "[[[-1.5288804094596773e+93, -1.5288804094596773e+93], -5.8803092671525835e+91], [[1.5288804094596773e+93, 1.5288804094596773e+93], 5.8803092671525835e+91]]\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "[[[0.0, 0.0], 0.0], [[0.0, 0.0], 0.0]]\n",
      "Forward\n",
      "[106, 106]\n",
      "[26, 26]\n",
      "[6, 6]\n"
     ]
    }
   ],
   "execution_count": 503
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T17:29:43.279485Z",
     "start_time": "2025-04-05T17:29:43.276100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion.batch_forward_backward(model, [y_true_sample_new], [X_sample_new])\n",
    "model.print_values(\"all\")"
   ],
   "id": "737313ba8d8a0e4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward_grad\n",
      "[[2.0, 2.0], [0.0, 0.0]]\n",
      "[[4.0, 4.0], [4.0, 4.0]]\n",
      "[[16.0, 16.0], [16.0, 16.0]]\n",
      "Grads:\n",
      "[[[26.0, 26.0], 1.0], [[0.0, 0.0], 0.0]]\n",
      "[[[12.0, 12.0], 2.0], [[12.0, 12.0], 2.0]]\n",
      "[[[8.0, 8.0], 8.0], [[8.0, 8.0], 8.0]]\n",
      "Batch_grad:\n",
      "[[[26.0, 26.0], 1.0], [[0.0, 0.0], 0.0]]\n",
      "[[[12.0, 12.0], 2.0], [[12.0, 12.0], 2.0]]\n",
      "[[[8.0, 8.0], 8.0], [[8.0, 8.0], 8.0]]\n",
      "Forward\n",
      "[106, 106]\n",
      "[26, 26]\n",
      "[6, 6]\n"
     ]
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T16:48:13.881Z",
     "start_time": "2025-04-05T16:48:13.877712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model([1, 2]))\n",
    "print(model.forward([1, 2]))\n",
    "model.backward()"
   ],
   "id": "e32467107cdc83f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138, 138]\n",
      "[138, 138]\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T16:36:05.545560Z",
     "start_time": "2025-04-05T16:36:05.543626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.add_grad_to_batch_grad()\n",
    "model.add_grad_to_batch_grad()\n",
    "model.add_grad_to_batch_grad()\n",
    "model.normalize_batch_grad(3)"
   ],
   "id": "f1630608776c1ec2",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T16:36:06.286601Z",
     "start_time": "2025-04-05T16:36:06.283401Z"
    }
   },
   "cell_type": "code",
   "source": "model.print_values(\"all\")",
   "id": "97495d9d9e4ef811",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward_grad\n",
      "[[2, 2], [2, 2]]\n",
      "[[8, 8], [8, 8]]\n",
      "[[32, 32], [32, 32]]\n",
      "Grads:\n",
      "[[[34, 34], 1], [[34, 34], 1]]\n",
      "[[[32, 32], 4], [[32, 32], 4]]\n",
      "[[[16, 32], 16], [[16, 32], 16]]\n",
      "Batch_grad:\n",
      "[[[45.333333333333336, 45.333333333333336], 1.3333333333333333], [[45.333333333333336, 45.333333333333336], 1.3333333333333333]]\n",
      "[[[42.666666666666664, 42.666666666666664], 5.333333333333333], [[42.666666666666664, 42.666666666666664], 5.333333333333333]]\n",
      "[[[21.333333333333332, 42.666666666666664], 21.333333333333332], [[21.333333333333332, 42.666666666666664], 21.333333333333332]]\n",
      "Forward\n",
      "[138, 138]\n",
      "[34, 34]\n",
      "[8, 8]\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:58:39.866306Z",
     "start_time": "2025-04-05T13:58:39.863860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(model.layers)):\n",
    "    number_layer = i\n",
    "    print(i, model.layers[number_layer].layer[0].backward_grad, model.layers[number_layer].layer[0].grad_weight,\n",
    "          model.layers[number_layer].layer[0].grad_bias)"
   ],
   "id": "4116e01b1f75c825",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [8, 8] [8, 16] 8\n",
      "1 [4, 4] [16, 16] 2\n",
      "2 [2, 2] [34, 34] 1\n"
     ]
    }
   ],
   "execution_count": 543
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T17:12:47.304801Z",
     "start_time": "2025-04-02T17:12:47.301826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = Perceptron(2)\n",
    "a([1, 2])"
   ],
   "id": "e7b908f3a8bc3e6f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.188139308930177"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T17:13:02.356072Z",
     "start_time": "2025-04-02T17:13:02.353485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a.forward([1,2])\n",
    "# print(a.value)"
   ],
   "id": "a06177ca50b7831e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.188139308930177\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T23:14:06.794500Z",
     "start_time": "2025-03-29T23:14:06.791657Z"
    }
   },
   "cell_type": "code",
   "source": "a.bias",
   "id": "a2c0347f8d36ac7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08249214176724436"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T11:37:03.231793Z",
     "start_time": "2025-04-02T11:37:03.229942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = None\n",
    "warnings.warn(f\"its {a}\")"
   ],
   "id": "890df32265413530",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/w406vwt16zl0wwv25rzht4fc0000gn/T/ipykernel_68565/3845773530.py:2: UserWarning: its None\n",
      "  warnings.warn(f\"its {a}\")\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T21:28:11.589204Z",
     "start_time": "2025-04-02T21:28:11.586903Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5c203e1f6dc8544f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T21:28:12.088760Z",
     "start_time": "2025-04-02T21:28:12.087361Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7680f55dc54d18dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T22:03:38.401889Z",
     "start_time": "2025-04-02T22:03:38.399719Z"
    }
   },
   "cell_type": "code",
   "source": "print([1, 2].reverse())",
   "id": "8ac91be27a2bb7dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T14:26:27.268007Z",
     "start_time": "2025-04-05T14:26:27.265370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = [1,2,3,]\n",
    "b = [10, 100, 1000]\n",
    "for i,j in zip(a,b):\n",
    "    j *= i\n",
    "    print(j)\n",
    "print(b)"
   ],
   "id": "ef55ca51461db939",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "200\n",
      "3000\n",
      "[10, 100, 1000]\n"
     ]
    }
   ],
   "execution_count": 568
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T14:26:18.052187Z",
     "start_time": "2025-04-05T14:26:18.049551Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1d8e709a371b2ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x10bfa3440>"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 567
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T17:45:42.524111Z",
     "start_time": "2025-04-05T17:45:42.522365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = \"e\"\n",
    "match a:\n",
    "    case \"xd\":\n",
    "        print(\"jdslfn\")\n",
    "    case \"x\"| \"d\":\n",
    "        print(\"kkkk\")"
   ],
   "id": "2e00e3d871ddb45c",
   "outputs": [],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T17:45:27.549270Z",
     "start_time": "2025-04-05T17:45:27.546328Z"
    }
   },
   "cell_type": "code",
   "source": "a",
   "id": "b6ff3b7b23aa5b4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('x', 'd')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:56:46.630747Z",
     "start_time": "2025-04-05T19:56:46.627522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lis = [Perceptron(2), Perceptron(1)]\n",
    "\n",
    "for i in range(len(lis)):\n",
    "    lis[i].backward_grad= 100\n",
    "\n",
    "lis[0].backward_grad"
   ],
   "id": "ce891c901c8820e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 517
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "532b8e1fcf5b62b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
